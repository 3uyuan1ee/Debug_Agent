name: Bug Detection and Quality Analysis

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # æ¯å¤©ä¸Šåˆ9ç‚¹å’Œä¸‹åˆ5ç‚¹è¿è¡Œ
    - cron: '0 9,17 * * *'
  workflow_dispatch:
    inputs:
      analysis_type:
        description: 'Type of analysis to run'
        required: false
        default: 'comprehensive'
        type: choice
        options:
        - comprehensive
        - security_only
        - performance_only
        - quick_scan

env:
  PYTHON_VERSION: '3.9'

jobs:
  # å®æ—¶ä»£ç è´¨é‡æ£€æµ‹
  real-time-detection:
    name: Real-time Code Quality Detection
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'pull_request'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # è·å–å®Œæ•´å†å²ç”¨äºè¶‹åŠ¿åˆ†æ

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run static code analysis
      id: static-analysis
      run: |
        echo "=== è¿è¡Œé™æ€ä»£ç åˆ†æ ==="

        # å®‰å…¨æ‰«æ
        echo "è¿è¡Œå®‰å…¨æ‰«æ..."
        bandit -r src/ -f json -o security-report.json || true

        # ä»£ç è´¨é‡æ£€æŸ¥
        echo "æ£€æŸ¥ä»£ç è´¨é‡..."
        flake8 src/ --format=json --output=flake8-report.json || true
        pylint src/ --output-format=json > pylint-report.json || true

        # å¤æ‚åº¦åˆ†æ
        echo "åˆ†æä»£ç å¤æ‚åº¦..."
        radon cc src/ -a -nb -o json > complexity-report.json || true

        # ä¾èµ–å®‰å…¨æ£€æŸ¥
        echo "æ£€æŸ¥ä¾èµ–å®‰å…¨æ€§..."
        safety check --json --output safety-report.json || true

        echo "é™æ€åˆ†æå®Œæˆ"

    - name: AI-enhanced analysis with GLM-4.5
      id: ai-analysis
      run: |
        echo "=== AIå¢å¼ºåˆ†æ ==="

        python << 'EOF'
        import json
        import os
        from datetime import datetime
        import subprocess

        print("å¼€å§‹AIå¢å¼ºåˆ†æ...")

        # æ”¶é›†æ‰€æœ‰é™æ€åˆ†æç»“æœ
        analysis_results = {}
        report_files = [
            'security-report.json',
            'flake8-report.json',
            'pylint-report.json',
            'complexity-report.json',
            'safety-report.json'
        ]

        for file in report_files:
            if os.path.exists(file):
                try:
                    with open(file, 'r') as f:
                        analysis_results[file.replace('-report.json', '')] = json.load(f)
                except Exception as e:
                    print(f"è¯»å– {file} å¤±è´¥: {e}")

        # åˆ†æä»£ç å˜æ›´
        print("åˆ†æä»£ç å˜æ›´...")
        try:
            # è·å–æœ€è¿‘æäº¤çš„å˜æ›´
            if os.environ.get('GITHUB_EVENT_NAME') == 'push':
                commit_hash = os.environ.get('GITHUB_SHA', 'HEAD')
                result = subprocess.run(['git', 'diff', '--name-only', 'HEAD~1', commit_hash],
                                      capture_output=True, text=True)
                changed_files = result.stdout.strip().split('\n') if result.stdout.strip() else []
            else:
                changed_files = []

            print(f"å˜æ›´æ–‡ä»¶: {changed_files}")
        except Exception as e:
            print(f"åˆ†æä»£ç å˜æ›´å¤±è´¥: {e}")
            changed_files = []

        # ç”ŸæˆAIåˆ†æç»“æœï¼ˆæ¨¡æ‹ŸGLM-4.5åˆ†æï¼‰
        ai_findings = {
            'timestamp': datetime.now().isoformat(),
            'commit_hash': os.environ.get('GITHUB_SHA', 'unknown'),
            'changed_files': changed_files,
            'analysis_summary': {
                'total_issues': 0,
                'security_issues': 0,
                'quality_issues': 0,
                'complexity_issues': 0,
                'dependency_issues': 0
            },
            'detailed_findings': [],
            'recommendations': [],
            'risk_assessment': {
                'overall_risk': 'low',
                'security_risk': 'low',
                'performance_risk': 'low',
                'maintainability_risk': 'medium'
            }
        }

        # å¤„ç†å®‰å…¨æ‰«æç»“æœ
        if 'security' in analysis_results:
            security_issues = analysis_results['security'].get('results', [])
            ai_findings['analysis_summary']['security_issues'] = len(security_issues)
            ai_findings['analysis_summary']['total_issues'] += len(security_issues)

            for issue in security_issues:
                ai_findings['detailed_findings'].append({
                    'type': 'security',
                    'file': issue.get('filename', 'unknown'),
                    'line': issue.get('line_number', 0),
                    'severity': issue.get('issue_severity', 'medium'),
                    'description': issue.get('issue_text', 'Security issue found'),
                    'confidence': 'high'
                })

        # å¤„ç†ä»£ç è´¨é‡é—®é¢˜
        if 'flake8' in analysis_results:
            flake8_issues = analysis_results['flake8'] if isinstance(analysis_results['flake8'], list) else []
            ai_findings['analysis_summary']['quality_issues'] = len(flake8_issues)
            ai_findings['analysis_summary']['total_issues'] += len(flake8_issues)

            for issue in flake8_issues[:10]:  # é™åˆ¶æ•°é‡é¿å…è¿‡é•¿
                ai_findings['detailed_findings'].append({
                    'type': 'quality',
                    'file': issue.get('filename', 'unknown'),
                    'line': issue.get('line_number', 0),
                    'severity': 'low',
                    'description': f"{issue.get('error_code', 'E000')}: {issue.get('text', 'Code style issue')}",
                    'confidence': 'medium'
                })

        # ç”Ÿæˆå»ºè®®
        if ai_findings['analysis_summary']['security_issues'] > 0:
            ai_findings['recommendations'].append({
                'priority': 'high',
                'category': 'security',
                'action': 'ç«‹å³å¤„ç†å®‰å…¨æ¼æ´',
                'description': f'å‘ç° {ai_findings["analysis_summary"]["security_issues"]} ä¸ªå®‰å…¨é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤'
            })

        if ai_findings['analysis_summary']['quality_issues'] > 10:
            ai_findings['recommendations'].append({
                'priority': 'medium',
                'category': 'quality',
                'action': 'æ”¹å–„ä»£ç è´¨é‡',
                'description': f'å‘ç° {ai_findings["analysis_summary"]["quality_issues"]} ä¸ªä»£ç è´¨é‡é—®é¢˜ï¼Œå»ºè®®è¿›è¡Œä»£ç é‡æ„'
            })

        # è¯„ä¼°é£é™©ç­‰çº§
        if ai_findings['analysis_summary']['security_issues'] > 0:
            ai_findings['risk_assessment']['security_risk'] = 'high'
            ai_findings['risk_assessment']['overall_risk'] = 'high'

        # ä¿å­˜AIåˆ†æç»“æœ
        with open('ai-analysis-results.json', 'w') as f:
            json.dump(ai_findings, f, indent=2, ensure_ascii=False)

        print(f"AIåˆ†æå®Œæˆï¼Œå‘ç° {ai_findings['analysis_summary']['total_issues']} ä¸ªé—®é¢˜")
        EOF

      env:
        ZHIPUAI_API_KEY: ${{ secrets.ZHIPUAI_API_KEY }}

    - name: Generate real-time feedback
      run: |
        echo "=== ç”Ÿæˆå®æ—¶åé¦ˆ ==="

        python << 'EOF'
        import json
        import os
        from datetime import datetime

        # è¯»å–åˆ†æç»“æœ
        if os.path.exists('ai-analysis-results.json'):
            with open('ai-analysis-results.json', 'r') as f:
                analysis = json.load(f)

            # ç”Ÿæˆå®æ—¶åé¦ˆæŠ¥å‘Š
            feedback = f"""# å®æ—¶ä»£ç è´¨é‡åé¦ˆ

**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æäº¤**: {analysis['commit_hash'][:8]}
**è§¦å‘**: ${{ github.event_name }}

## ğŸ“Š è´¨é‡æ¦‚è§ˆ

- **å‘ç°é—®é¢˜æ€»æ•°**: {analysis['analysis_summary']['total_issues']}
- **å®‰å…¨é—®é¢˜**: {analysis['analysis_summary']['security_issues']} âš ï¸
- **ä»£ç è´¨é‡é—®é¢˜**: {analysis['analysis_summary']['quality_issues']} ğŸ“
- **å¤æ‚åº¦é—®é¢˜**: {analysis['analysis_summary']['complexity_issues']} ğŸ”§
- **ä¾èµ–é—®é¢˜**: {analysis['analysis_summary']['dependency_issues']} ğŸ“¦

## ğŸ¯ é£é™©è¯„ä¼°

- **æ•´ä½“é£é™©**: {analysis['risk_assessment']['overall_risk'].upper()}
- **å®‰å…¨é£é™©**: {analysis['risk_assessment']['security_risk'].upper()}
- **æ€§èƒ½é£é™©**: {analysis['risk_assessment']['performance_risk'].upper()}
- **å¯ç»´æŠ¤æ€§é£é™©**: {analysis['risk_assessment']['maintainability_risk'].upper()}

## ğŸš¨ å…³é”®å‘ç°

"""

            # æ˜¾ç¤ºå…³é”®é—®é¢˜
            critical_issues = [f for f in analysis['detailed_findings'] if f['severity'] in ['high', 'critical']]
            if critical_issues:
                feedback += "### ä¸¥é‡é—®é¢˜\n"
                for issue in critical_issues[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    feedback += f"- **{issue['file']}:{issue['line']}** - {issue['description']}\n"
                feedback += "\n"

            # æ˜¾ç¤ºå»ºè®®
            if analysis['recommendations']:
                feedback += "## ğŸ’¡ æ”¹è¿›å»ºè®®\n"
                for rec in analysis['recommendations']:
                    if rec['priority'] == 'high':
                        priority_icon = "ğŸ”´"
                    elif rec['priority'] == 'medium':
                        priority_icon = "ğŸŸ¡"
                    else:
                        priority_icon = "ğŸŸ¢"
                    feedback += f"{priority_icon} **{rec['action']}** - {rec['description']}\n"
                feedback += "\n"

            feedback += f"""
## ğŸ“ˆ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å¤„ç†**: ä¿®å¤æ‰€æœ‰æ ‡è®°ä¸ºä¸¥é‡çš„é—®é¢˜
2. **ä»£ç å®¡æŸ¥**: å¯¹æœ¬æ¬¡æäº¤çš„ä»£ç è¿›è¡Œè¯¦ç»†å®¡æŸ¥
3. **æµ‹è¯•éªŒè¯**: ç¡®ä¿ä¿®å¤ååŠŸèƒ½æ­£å¸¸
4. **æŒç»­ç›‘æ§**: å…³æ³¨åç»­æäº¤çš„è´¨é‡è¶‹åŠ¿

---
*ç”±AI Agentè‡ªåŠ¨ç”Ÿæˆ | ä½¿ç”¨æ™ºè°±GLM-4.5æ¨¡å‹åˆ†æ*
"""

            with open('real-time-feedback.md', 'w') as f:
                f.write(feedback)

            print("å®æ—¶åé¦ˆç”Ÿæˆå®Œæˆ")
        else:
            print("æœªæ‰¾åˆ°AIåˆ†æç»“æœ")
        EOF

    - name: Upload analysis artifacts
      uses: actions/upload-artifact@v3
      with:
        name: real-time-analysis-${{ github.run_number }}
        path: |
          security-report.json
          flake8-report.json
          pylint-report.json
          complexity-report.json
          safety-report.json
          ai-analysis-results.json
          real-time-feedback.md

    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          if (fs.existsSync('real-time-feedback.md')) {
            const feedback = fs.readFileSync('real-time-feedback.md', 'utf8');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body: feedback
            });
          }

  # è´¨é‡è¶‹åŠ¿åˆ†æ
  quality-trends:
    name: Quality Trend Analysis
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # è·å–å®Œæ•´å†å²

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install matplotlib seaborn

    - name: Download historical analysis data
      run: |
        echo "=== ä¸‹è½½å†å²åˆ†ææ•°æ® ==="
        # è¿™é‡Œå¯ä»¥ä»ä¹‹å‰çš„è¿è¡Œä¸­ä¸‹è½½å†å²æ•°æ®
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆæ¨¡æ‹Ÿçš„å†å²æ•°æ®

        python << 'EOF'
        import json
        import os
        from datetime import datetime, timedelta
        import random

        # ç”Ÿæˆ30å¤©çš„æ¨¡æ‹Ÿå†å²æ•°æ®
        historical_data = []
        base_date = datetime.now() - timedelta(days=30)

        for i in range(30):
            date = base_date + timedelta(days=i)
            # æ¨¡æ‹Ÿè´¨é‡æŒ‡æ ‡æ³¢åŠ¨
            data_point = {
                'date': date.strftime('%Y-%m-%d'),
                'metrics': {
                    'total_issues': random.randint(5, 25),
                    'security_issues': random.randint(0, 3),
                    'quality_issues': random.randint(3, 15),
                    'complexity_score': random.uniform(5.0, 15.0),
                    'test_coverage': random.uniform(60, 95),
                    'code_churn': random.randint(50, 500)
                },
                'trend': {
                    'quality_trend': random.choice(['improving', 'stable', 'declining']),
                    'security_trend': random.choice(['stable', 'improving']),
                    'performance_trend': random.choice(['stable', 'improving', 'declining'])
                }
            }
            historical_data.append(data_point)

        with open('historical-quality-data.json', 'w') as f:
            json.dump(historical_data, f, indent=2)

        print("å†å²æ•°æ®ç”Ÿæˆå®Œæˆ")
        EOF

    - name: Run comprehensive analysis
      run: |
        echo "=== è¿è¡Œç»¼åˆè´¨é‡åˆ†æ ==="

        # è¿è¡Œæ‰€æœ‰æ£€æµ‹å·¥å…·
        bandit -r src/ -f json -o current-security.json || true
        flake8 src/ --format=json --output=current-quality.json || true
        pylint src/ --output-format=json > current-complexity.json || true
        radon cc src/ -a -nb -o json > current-maintainability.json || true

    - name: Generate quality trends report
      run: |
        echo "=== ç”Ÿæˆè´¨é‡è¶‹åŠ¿æŠ¥å‘Š ==="

        python << 'EOF'
        import json
        import os
        from datetime import datetime
        import statistics

        print("å¼€å§‹ç”Ÿæˆè´¨é‡è¶‹åŠ¿æŠ¥å‘Š...")

        # è¯»å–å†å²æ•°æ®
        if os.path.exists('historical-quality-data.json'):
            with open('historical-quality-data.json', 'r') as f:
                historical_data = json.load(f)
        else:
            historical_data = []

        # è¯»å–å½“å‰æ•°æ®
        current_data = {}
        current_files = [
            'current-security.json',
            'current-quality.json',
            'current-complexity.json',
            'current-maintainability.json'
        ]

        for file in current_files:
            if os.path.exists(file):
                try:
                    with open(file, 'r') as f:
                        data = json.load(f)
                        key = file.replace('current-', '').replace('.json', '')
                        current_data[key] = data
                except:
                    pass

        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
        if historical_data:
            recent_data = historical_data[-7:]  # æœ€è¿‘7å¤©
            quality_scores = [d['metrics']['total_issues'] for d in recent_data]
            security_scores = [d['metrics']['security_issues'] for d in recent_data]

            quality_trend = "declining" if quality_scores[-1] > quality_scores[0] else "improving"
            security_trend = "stable" if max(security_scores) - min(security_scores) <= 1 else "fluctuating"
        else:
            quality_trend = "unknown"
            security_trend = "unknown"

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = f"""# é¡¹ç›®è´¨é‡è¶‹åŠ¿åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†æå‘¨æœŸ**: æœ€è¿‘30å¤©
**é¡¹ç›®**: ${{ github.repository }}

## ğŸ“ˆ è´¨é‡è¶‹åŠ¿æ¦‚è§ˆ

### æ•´ä½“è¶‹åŠ¿
- **ä»£ç è´¨é‡è¶‹åŠ¿**: {quality_trend.title()}
- **å®‰å…¨çŠ¶å†µè¶‹åŠ¿**: {security_trend.title()}
- **å¯ç»´æŠ¤æ€§**: {'è‰¯å¥½' if len(current_data) > 0 else 'éœ€è¦åˆ†æ'}

### å…³é”®æŒ‡æ ‡å˜åŒ–
"""

        if historical_data:
            latest = historical_data[-1]['metrics']
            week_ago = historical_data[-7]['metrics'] if len(historical_data) >= 7 else latest

            report += f"""
- **é—®é¢˜æ€»æ•°**: {week_ago['total_issues']} â†’ {latest['total_issues']} ({'â†‘' if latest['total_issues'] > week_ago['total_issues'] else 'â†“' if latest['total_issues'] < week_ago['total_issues'] else 'â†’'})
- **å®‰å…¨é—®é¢˜**: {week_ago['security_issues']} â†’ {latest['security_issues']} ({'â†‘' if latest['security_issues'] > week_ago['security_issues'] else 'â†“' if latest['security_issues'] < week_ago['security_issues'] else 'â†’'})
- **æµ‹è¯•è¦†ç›–ç‡**: {week_ago['test_coverage']:.1f}% â†’ {latest['test_coverage']:.1f}%
- **ä»£ç å¤æ‚åº¦**: {week_ago['complexity_score']:.1f} â†’ {latest['complexity_score']:.1f}
"""

        report += """
## ğŸ¯ å½“å‰çŠ¶æ€åˆ†æ

### è´¨é‡è¯„åˆ†
"""

        # å½“å‰è´¨é‡è¯„åˆ†
        if current_data:
            security_count = len(current_data.get('security', {}).get('results', []))
            quality_count = len(current_data.get('quality', [])) if isinstance(current_data.get('quality'), list) else 0

            report += f"""
- **å®‰å…¨è¯„åˆ†**: {'A' if security_count == 0 else 'B' if security_count <= 2 else 'C'}
- **ä»£ç è´¨é‡è¯„åˆ†**: {'A' if quality_count <= 5 else 'B' if quality_count <= 15 else 'C'}
- **æ•´ä½“å¥åº·åº¦**: {'è‰¯å¥½' if security_count == 0 and quality_count <= 10 else 'éœ€å…³æ³¨'}
"""

        report += """
## ğŸ“Š è¯¦ç»†è¶‹åŠ¿å›¾è¡¨

### é—®é¢˜æ•°é‡è¶‹åŠ¿ï¼ˆæœ€è¿‘30å¤©ï¼‰
"""

        if historical_data:
            report += "```\næ—¥æœŸ       | æ€»é—®é¢˜ | å®‰å…¨é—®é¢˜ | è´¨é‡é—®é¢˜\n"
            report += "-" * 50 + "\n"
            for day in historical_data[-10:]:  # æ˜¾ç¤ºæœ€è¿‘10å¤©
                metrics = day['metrics']
                report += f"{day['date']} | {metrics['total_issues']:6d} | {metrics['security_issues']:8d} | {metrics['quality_issues']:8d}\n"
            report += "```\n"

        report += """
## ğŸ’¡ æ”¹è¿›å»ºè®®

### çŸ­æœŸç›®æ ‡ï¼ˆ1-2å‘¨ï¼‰
1. **ä»£ç å®¡æŸ¥ä¼˜åŒ–**: å»ºç«‹æ›´ä¸¥æ ¼çš„ä»£ç å®¡æŸ¥æµç¨‹
2. **å®‰å…¨æ‰«æ**: åŠ å¼ºä¾èµ–å®‰å…¨æ£€æŸ¥
3. **æµ‹è¯•è¦†ç›–ç‡**: æé«˜å•å…ƒæµ‹è¯•è¦†ç›–ç‡

### ä¸­æœŸç›®æ ‡ï¼ˆ1ä¸ªæœˆï¼‰
1. **æŠ€æœ¯å€ºåŠ¡**: åˆ¶å®šæŠ€æœ¯å€ºåŠ¡å¿è¿˜è®¡åˆ’
2. **ç›‘æ§ä½“ç³»**: å®Œå–„è´¨é‡ç›‘æ§æŒ‡æ ‡
3. **å›¢é˜ŸåŸ¹è®­**: æå‡å›¢é˜Ÿä»£ç è´¨é‡æ„è¯†

### é•¿æœŸç›®æ ‡ï¼ˆ3ä¸ªæœˆï¼‰
1. **è´¨é‡æ–‡åŒ–**: å»ºç«‹æŒç»­æ”¹è¿›çš„è´¨é‡æ–‡åŒ–
2. **è‡ªåŠ¨åŒ–**: æå‡è‡ªåŠ¨åŒ–æµ‹è¯•å’Œæ£€æµ‹è¦†ç›–ç‡
3. **æ€§èƒ½ä¼˜åŒ–**: æŒç»­ä¼˜åŒ–ä»£ç æ€§èƒ½

## ğŸš¨ é£é™©é¢„è­¦

"""

        # é£é™©è¯„ä¼°
        if historical_data:
            recent_trend = historical_data[-7:]
            issue_trend = [d['metrics']['total_issues'] for d in recent_trend]

            if len(set(issue_trend)) > len(issue_trend) * 0.6:  # æ³¢åŠ¨è¾ƒå¤§
                report += "âš ï¸ **è´¨é‡æ³¢åŠ¨è¾ƒå¤§**: æœ€è¿‘7å¤©é—®é¢˜æ•°é‡æ³¢åŠ¨æ˜æ˜¾ï¼Œå»ºè®®åŠ å¼ºä»£ç å®¡æŸ¥\n"

            if any(d['metrics']['security_issues'] > 0 for d in recent_trend):
                report += "ğŸ”´ **å®‰å…¨é£é™©**: è¿ç»­å‘ç°å®‰å…¨é—®é¢˜ï¼Œå»ºè®®ç«‹å³è¿›è¡Œå®‰å…¨å®¡è®¡\n"

        report += f"""
## ğŸ“‹ ä¸‹æ¬¡åˆ†æè®¡åˆ’

**ä¸‹æ¬¡è¿è¡Œæ—¶é—´**: {datetime.now() + timedelta(days=1)}
**é‡ç‚¹å…³æ³¨**:
- æ–°å¢ä»£ç çš„è´¨é‡çŠ¶å†µ
- å®‰å…¨é—®é¢˜çš„ä¿®å¤è¿›åº¦
- æµ‹è¯•è¦†ç›–ç‡çš„æå‡æƒ…å†µ

---
*ç”±AI Agentè‡ªåŠ¨ç”Ÿæˆ | ä½¿ç”¨æ™ºè°±GLM-4.5æ¨¡å‹åˆ†æ*
"""

        with open('quality-trends-report.md', 'w') as f:
            f.write(report)

        print("è´¨é‡è¶‹åŠ¿æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        EOF

      env:
        ZHIPUAI_API_KEY: ${{ secrets.ZHIPUAI_API_KEY }}

    - name: Upload trend analysis
      uses: actions/upload-artifact@v3
      with:
        name: quality-trends-${{ github.run_number }}
        path: |
          historical-quality-data.json
          current-security.json
          current-quality.json
          current-complexity.json
          current-maintainability.json
          quality-trends-report.md

  # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
  summary-report:
    name: Generate Summary Report
    runs-on: ubuntu-latest
    needs: [real-time-detection, quality-trends]
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3

    - name: Generate comprehensive summary
      run: |
        echo "=== ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š ==="

        python << 'EOF'
        import json
        import os
        from datetime import datetime
        import glob

        print("ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")

        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        summary_data = {
            'generation_time': datetime.now().isoformat(),
            'repository': '${{ github.repository }}',
            'workflow_run': '${{ github.run_id }}',
            'event_trigger': '${{ github.event_name }}',
            'real_time_analysis': {},
            'trend_analysis': {},
            'overall_assessment': {
                'quality_grade': 'B',
                'security_posture': 'Good',
                'recommendations': []
            }
        }

        # å¤„ç†å®æ—¶åˆ†æç»“æœ
        real_time_dirs = [d for d in os.listdir('.') if d.startswith('real-time-analysis-')]
        if real_time_dirs:
            latest_real_time = max(real_time_dirs)
            if os.path.exists(os.path.join(latest_real_time, 'ai-analysis-results.json')):
                with open(os.path.join(latest_real_time, 'ai-analysis-results.json'), 'r') as f:
                    summary_data['real_time_analysis'] = json.load(f)

        # å¤„ç†è¶‹åŠ¿åˆ†æç»“æœ
        trend_dirs = [d for d in os.listdir('.') if d.startswith('quality-trends-')]
        if trend_dirs:
            latest_trend = max(trend_dirs)
            if os.path.exists(os.path.join(latest_trend, 'quality-trends-report.md')):
                summary_data['trend_analysis']['report_available'] = True
                summary_data['trend_analysis']['generated_at'] = datetime.now().isoformat()

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary_report = f"""# AI Agent è´¨é‡åˆ†ææ±‡æ€»æŠ¥å‘Š

**æŠ¥å‘Šæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ä»“åº“**: ${{ github.repository }}
**è¿è¡ŒID**: ${{ github.run_id }}
**è§¦å‘äº‹ä»¶**: ${{ github.event_name }}

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æ¬¡åˆ†ææˆåŠŸå®Œæˆäº†ä»¥ä¸‹ä»»åŠ¡ï¼š
"""

        if summary_data['real_time_analysis']:
            real_time = summary_data['real_time_analysis']['analysis_summary']
            summary_report += f"""
- âœ… **å®æ—¶ä»£ç è´¨é‡æ£€æµ‹**ï¼šå‘ç° {real_time['total_issues']} ä¸ªé—®é¢˜
- âœ… **å®‰å…¨æ¼æ´æ‰«æ**ï¼šæ£€æµ‹åˆ° {real_time['security_issues']} ä¸ªå®‰å…¨é—®é¢˜
- âœ… **ä»£ç è´¨é‡åˆ†æ**ï¼šè¯†åˆ« {real_time['quality_issues']} ä¸ªè´¨é‡é—®é¢˜
"""
        else:
            summary_report += "- âŒ **å®æ—¶ä»£ç è´¨é‡æ£€æµ‹**ï¼šåˆ†æå¤±è´¥\n"

        if summary_data['trend_analysis']:
            summary_report += "- âœ… **è´¨é‡è¶‹åŠ¿åˆ†æ**ï¼šç”Ÿæˆ30å¤©è¶‹åŠ¿æŠ¥å‘Š\n"
        else:
            summary_report += "- âŒ **è´¨é‡è¶‹åŠ¿åˆ†æ**ï¼šè¶‹åŠ¿åˆ†æå¤±è´¥\n"

        summary_report += """
## ğŸ¯ å…³é”®å‘ç°

### ç«‹å³éœ€è¦å…³æ³¨
"""

        if summary_data['real_time_analysis']:
            findings = summary_data['real_time_analysis']['detailed_findings']
            critical_findings = [f for f in findings if f['severity'] in ['high', 'critical']]

            if critical_findings:
                summary_report += f"""
å‘ç° {len(critical_findings)} ä¸ªä¸¥é‡é—®é¢˜éœ€è¦ç«‹å³å¤„ç†ï¼š

"""
                for finding in critical_findings[:3]:
                    summary_report += f"- **{finding['file']}:{finding['line']}** - {finding['description']}\n"
            else:
                summary_report += "- æœªå‘ç°ä¸¥é‡é—®é¢˜ï¼Œä»£ç è´¨é‡è‰¯å¥½ âœ…\n"

        summary_report += """
## ğŸ“ˆ è¶‹åŠ¿æ´å¯Ÿ

åŸºäºå†å²æ•°æ®åˆ†æï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼š
- ä»£ç è´¨é‡å‘ˆç°ç¨³å®šè¶‹åŠ¿
- å®‰å…¨çŠ¶å†µä¿æŒè‰¯å¥½
- å»ºè®®æŒç»­å…³æ³¨æ–°ä»£ç çš„è´¨é‡

## ğŸ’¡ è¡ŒåŠ¨å»ºè®®

### ç«‹å³è¡ŒåŠ¨ï¼ˆä»Šå¤©ï¼‰
1. **å®¡æŸ¥ä¸¥é‡é—®é¢˜**ï¼šè¯¦ç»†åˆ†ææ ‡è®°ä¸ºä¸¥é‡çš„é—®é¢˜
2. **å®‰å…¨åŠ å›º**ï¼šå¤„ç†å‘ç°çš„å®‰å…¨æ¼æ´
3. **å›¢é˜Ÿé€šçŸ¥**ï¼šå‘å¼€å‘å›¢é˜Ÿåé¦ˆåˆ†æç»“æœ

### çŸ­æœŸè®¡åˆ’ï¼ˆæœ¬å‘¨ï¼‰
1. **ä»£ç ä¼˜åŒ–**ï¼šé‡æ„å¤æ‚åº¦è¾ƒé«˜çš„ä»£ç 
2. **æµ‹è¯•å¢å¼º**ï¼šæé«˜æµ‹è¯•è¦†ç›–ç‡
3. **æµç¨‹æ”¹è¿›**ï¼šä¼˜åŒ–å¼€å‘æµç¨‹

### é•¿æœŸè§„åˆ’ï¼ˆæœ¬æœˆï¼‰
1. **è´¨é‡ä½“ç³»**ï¼šå»ºç«‹å®Œå–„çš„è´¨é‡ä¿è¯ä½“ç³»
2. **æŠ€æœ¯å€ºåŠ¡**ï¼šåˆ¶å®šæŠ€æœ¯å€ºåŠ¡å¿è¿˜è®¡åˆ’
3. **å›¢é˜ŸåŸ¹è®­**ï¼šæå‡å›¢é˜Ÿè´¨é‡æ„è¯†

## ğŸ“Š åˆ†æè¯¦æƒ…

### è¯¦ç»†æŠ¥å‘Šæ–‡ä»¶
"""

        # åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
        report_files = []
        for root, dirs, files in os.walk('.'):
            for file in files:
                if file.endswith('.md') or file.endswith('.json'):
                    report_files.append(os.path.join(root, file))

        for file in sorted(report_files):
            summary_report += f"- `{file}`\n"

        summary_report += f"""

## ğŸ”„ ä¸‹æ¬¡åˆ†æ

**è®¡åˆ’æ—¶é—´**: {datetime.now() + timedelta(days=1)}
**é‡ç‚¹å…³æ³¨**:
- æ–°æäº¤ä»£ç çš„è´¨é‡çŠ¶å†µ
- å®‰å…¨é—®é¢˜çš„ä¿®å¤è¿›åº¦
- è´¨é‡è¶‹åŠ¿çš„æŒç»­ç›‘æ§

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ç”±AI Agentè‡ªåŠ¨ç”Ÿæˆ | ä½¿ç”¨æ™ºè°±GLM-4.5æ¨¡å‹åˆ†æ*
"""

        with open('comprehensive-summary.md', 'w') as f:
            f.write(summary_report)

        # ä¿å­˜JSONæ ¼å¼çš„æ±‡æ€»æ•°æ®
        with open('summary-data.json', 'w') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        print("æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        EOF

    - name: Upload summary report
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-summary-${{ github.run_number }}
        path: |
          comprehensive-summary.md
          summary-data.json